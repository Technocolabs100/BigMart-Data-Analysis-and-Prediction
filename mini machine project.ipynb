{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fee19a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings  # Ignores any warning\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "# print(train.head())\n",
    "# train.info()\n",
    "\"\"\" On seeing the data information, we see that there are many null values, thus we have to remove the 0 values\n",
    "\n",
    "# Check for duplicates\n",
    "idsUnique = len(set(train.Item_Identifier))\n",
    "idsTotal = data.shape[0]\n",
    "idsDupli = idsTotal - idsUnique\n",
    "\n",
    "print(\"There are \" + str(idsDupli) + \" duplicate IDs for \" + str(idsTotal) + \" total entries\")\"\"\"\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.distplot(train.Item_Outlet_Sales, bins=25)\n",
    "plt.ticklabel_format(style='plain', axis='x', scilimits=(0, 1))\n",
    "plt.xlabel(\"Item_Outlet_Sales\")\n",
    "plt.ylabel(\"Number of Sales\")\n",
    "plt.title(\"Item_Outlet_Sales Distribution\")\n",
    "plt.show()  # Distribution of item outlet sales\n",
    "\n",
    "sns.countplot(train.Item_Fat_Content)\n",
    "plt.show()  # Distribution of fat content\n",
    "\n",
    "sns.countplot(train.Item_Type)\n",
    "plt.xticks(rotation=90)\n",
    "sns.countplot(train.Outlet_Size)\n",
    "plt.show()  # Distribution of outlet size\n",
    "\n",
    "sns.countplot(train.Outlet_Location_Type)\n",
    "plt.show()  # Distribution of outlet location type\n",
    "\n",
    "sns.countplot(train.Outlet_Type)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()  # Distribution of outlet type\n",
    "\n",
    "# Join Train and Test Dataset\n",
    "\n",
    "# Create source column to later separate the data easily\n",
    "train['source'] = 'train'\n",
    "test['source'] = 'test'\n",
    "\n",
    "data = pd.concat([train, test], ignore_index=True)\n",
    "print(train.shape, test.shape, data.shape)\n",
    "# aggfunc is mean by default and ignores NaN by default\n",
    "\n",
    "item_avg_weight = data.pivot_table(values='Item_Weight', index='Item_Identifier')  # Create table with identifiers and its weights and ignores the NaN values\n",
    "\n",
    "\n",
    "def impute_weight(cols):  # Filling null values with mean\n",
    "    Weight = cols[0]\n",
    "    Identifier = cols[1]\n",
    "\n",
    "    if pd.isnull(Weight):\n",
    "        return item_avg_weight['Item_Weight'][item_avg_weight.index == Identifier]\n",
    "    else:\n",
    "        return Weight\n",
    "\n",
    "\n",
    "print ('Orignal missing: %d' % sum(data['Item_Weight'].isnull()))\n",
    "print (data['Item_Weight'].mean())\n",
    "\n",
    "data['Item_Weight'] = data[['Item_Weight', 'Item_Identifier']].apply(impute_weight, axis=1).astype(float)\n",
    "\n",
    "print ('Final missing: %d' % sum(data['Item_Weight'].isnull()))\n",
    "\n",
    "#print (data['Item_Weight'].mean())\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Determing the mode for each\n",
    "outlet_size_mode = data.pivot_table(values='Outlet_Size', columns='Outlet_Type', aggfunc=lambda x: x.mode())\n",
    "\n",
    "\n",
    "def impute_size_mode(cols):  # Same as above but using mode\n",
    "    Size = cols[0]\n",
    "    Type = cols[1]\n",
    "    if pd.isnull(Size):\n",
    "        return outlet_size_mode.loc['Outlet_Size'][outlet_size_mode.columns == Type][0]\n",
    "    else:\n",
    "        return Size\n",
    "\n",
    "\n",
    "print ('Orignal missing: %d' % sum(data['Outlet_Size'].isnull()))\n",
    "data['Outlet_Size'] = data[['Outlet_Size', 'Outlet_Type']].apply(impute_size_mode, axis=1)\n",
    "print ('Final missing: %d' % sum(data['Outlet_Size'].isnull()))\n",
    "\n",
    "visibility_item_avg = data.pivot_table(values='Item_Visibility', index='Item_Identifier')\n",
    "\n",
    "\n",
    "def impute_visibility_mean(cols):\n",
    "    visibility = cols[0]\n",
    "    item = cols[1]\n",
    "    if visibility == 0:\n",
    "        return visibility_item_avg['Item_Visibility'][visibility_item_avg.index == item]\n",
    "    else:\n",
    "        return visibility\n",
    "\n",
    "\n",
    "print ('Original zeros: %d' % sum(data['Item_Visibility'] == 0))\n",
    "data['Item_Visibility'] = data[['Item_Visibility', 'Item_Identifier']].apply(impute_visibility_mean, axis=1).astype(float)\n",
    "\n",
    "print ('Final zeros: %d' % sum(data['Item_Visibility'] == 0))\n",
    "# Remember the data is from 2013\n",
    "data['Outlet_Years'] = 2013 - data['Outlet_Establishment_Year']\n",
    "data['Outlet_Years'].describe()\n",
    "\n",
    "# Get the first two characters of ID:\n",
    "data['Item_Type_Combined'] = data['Item_Identifier'].apply(lambda x: x[0:2])\n",
    "\n",
    "# Rename them to more intuitive categories:\n",
    "data['Item_Type_Combined'] = data['Item_Type_Combined'].map({'FD': 'Food', 'NC': 'Non-Consumable', 'DR': 'Drinks'})\n",
    "\n",
    "data['Item_Type_Combined'].value_counts()\n",
    "\n",
    "data['Item_Fat_Content'] = data['Item_Fat_Content'].replace({'LF': 'Low Fat', 'reg': 'Regular',\n",
    "                                                             'low fat': 'Low Fat'})\n",
    "\n",
    "# print(data['Item_Fat_Content'].value_counts())\n",
    "\n",
    "# Mark non-consumables as separate category in low_fat:\n",
    "\n",
    "data.loc[data['Item_Type_Combined'] == \"Non-Consumable\", 'Item_Fat_Content'] = \"Non-Edible\"\n",
    "\n",
    "data['Item_Fat_Content'].value_counts()\n",
    "\n",
    "\n",
    "def func(x): return x['Item_Visibility'] / visibility_item_avg['Item_Visibility'][visibility_item_avg.index == x['Item_Identifier']][0]\n",
    "\n",
    "\n",
    "data['Item_Visibility_MeanRatio'] = data.apply(func, axis=1).astype(float)\n",
    "data['Item_Visibility_MeanRatio'].describe()\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# New variable for outlet\n",
    "data['Outlet'] = le.fit_transform(data['Outlet_Identifier'])\n",
    "\n",
    "var_mod = ['Item_Fat_Content', 'Outlet_Location_Type', 'Outlet_Size', 'Item_Type_Combined', 'Outlet_Type', 'Outlet']\n",
    "\n",
    "for i in var_mod:\n",
    "    data[i] = le.fit_transform(data[i])\n",
    "\n",
    "# Exporting the data back\n",
    "\n",
    "# Drop the columns which have been converted to different types:\n",
    "data.drop(['Item_Type', 'Outlet_Establishment_Year'], axis=1, inplace=True)\n",
    "\n",
    "# Divide into test and train:\n",
    "train = data.loc[data['source'] == \"train\"]\n",
    "test = data.loc[data['source'] == \"test\"]\n",
    "\n",
    "# Drop unnecessary columns:\n",
    "test.drop(['Item_Outlet_Sales', 'source'], axis=1, inplace=True)\n",
    "train.drop(['source'], axis=1, inplace=True)\n",
    "\n",
    "# Export files as modified versions:\n",
    "train.to_csv(\"data/train_modified.csv\", index=False)\n",
    "test.to_csv(\"data/test_modified.csv\", index=False)\n",
    "\n",
    "train_df = pd.read_csv('data/train_modified.csv')\n",
    "test_df = pd.read_csv('data/test_modified.csv')\n",
    "\n",
    "# Define target and ID columns:\n",
    "target = 'Item_Outlet_Sales'\n",
    "IDcol = ['Item_Identifier', 'Outlet_Identifier']\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def modelfit(alg, dtrain, dtest, predictors, target, IDcol, filename):\n",
    "    # Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[target])\n",
    "\n",
    "    # Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "\n",
    "    # Remember the target had been normalized\n",
    "    Sq_train = (dtrain[target])**2\n",
    "\n",
    "# Perform cross-validation:\n",
    "    cv_score = cross_validation.cross_val_score(alg, dtrain[predictors], Sq_train, cv=20, scoring='neg_mean_squared_error')\n",
    "    cv_score = np.sqrt(np.abs(cv_score))\n",
    "\n",
    "    # Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"RMSE : %.4g\" % np.sqrt(metrics.mean_squared_error(Sq_train.values, dtrain_predictions)))\n",
    "\n",
    "    print(\"CV Score : Mean - %.4g | Std - %.4g | Min - %.4g | Max - %.4g\" % (np.mean(cv_score), np.std(cv_score), np.min(cv_score), np.max(cv_score)))\n",
    "\n",
    "    # Predict on testing data:\n",
    "    dtest[target] = alg.predict(dtest[predictors])\n",
    "\n",
    "    # Export submission file:\n",
    "    IDcol.append(target)\n",
    "    submission = pd.DataFrame({x: dtest[x] for x in IDcol})\n",
    "    submission.to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "# Linear Regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "LR = LinearRegression(normalize=True)\n",
    "\n",
    "predictors = train_df.columns.drop(['Item_Outlet_Sales', 'Item_Identifier', 'Outlet_Identifier'])\n",
    "modelfit(LR, train_df, test_df, predictors, target, IDcol, 'LR.csv')\n",
    "# Decision Tree model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "DT = DecisionTreeRegressor(max_depth=15, min_samples_leaf=100)\n",
    "modelfit(DT, train_df, test_df, predictors, target, IDcol, \"DT.csv\")\n",
    "\n",
    "RF = DecisionTreeRegressor(max_depth=8, min_samples_leaf=150)\n",
    "modelfit(RF, train_df, test_df, predictors, target, IDcol, \"RF.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "EusaZy_DTcri",
        "KWsFqyZ9eX-p",
        "aqmKyo9gewGn",
        "OlqdCZome_NA",
        "okOcnTEbGH7d",
        "Hwr9wcWJq59d",
        "IDjDoa-rrFzV",
        "q2W6KSDaEvBP",
        "yXRq4gNoFf1B",
        "W-aCol3iE1KJ"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPUwfoUdPYFMyqRenvesy3s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benisalla/BigMart-Data-Analysis-and-Prediction/blob/main/GPT_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading Dummy Data for test"
      ],
      "metadata": {
        "id": "EusaZy_DTcri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfXcoP07TrAF",
        "outputId": "85a435a4-1738-4b10-821f-9e6252c7a0ad"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "path = \"/content/drive/MyDrive/translator-first-try/europarl-v7.fr-en.en\"\n",
        "with open(path, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "text[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "F8_pWbqLThOi",
        "outputId": "0dfd159e-e225-4461-f9df-880df6640e2f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Resumption of the session\\nI declare resumed the session of the European Parliament adjourned on Frid'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(f\"vocab_size : {vocab_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-Rbyr8sfM8",
        "outputId": "824fd6ec-851f-4380-cd3b-7703151dbc70"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size : 315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "word2number = { ch:i for i,ch in enumerate(chars) }\n",
        "number2word = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [word2number[c] for c in s]\n",
        "decode = lambda l: ''.join([number2word[i] for i in l])\n",
        "\n",
        "print(encode(\"I love neural networks\"))\n",
        "print(decode(encode(\"I love neural networks\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1LKNCgwjj1",
        "outputId": "f00c5aff-1ebc-482a-dfd0-8c5b2a4e96c4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[42, 1, 76, 79, 86, 69, 1, 78, 69, 85, 82, 65, 76, 1, 78, 69, 84, 87, 79, 82, 75, 83]\n",
            "I love neural networks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "text_data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(text_data.shape, text_data.dtype)\n",
        "print(text_data[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "33873577-90b6-4b01-a2b4-4ee1689a4105"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([301210536]) torch.int64\n",
            "tensor([51, 69, 83, 85, 77, 80, 84, 73, 79, 78])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing required libraries."
      ],
      "metadata": {
        "id": "KWsFqyZ9eX-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install wandb"
      ],
      "metadata": {
        "id": "6UZFoU3R_uq1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "import numpy as np\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "import wandb"
      ],
      "metadata": {
        "id": "BUlpvHPV3yLI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The Core of GPT-2\n",
        "\n",
        "<center>\n",
        "  <img src=\"https://github-production-user-asset-6210df.s3.amazonaws.com/89405673/258571749-4954bee7-47e4-4848-a842-affc25c3cc17.png\" width=\"900\" height=\"260\"/>\n",
        "</center>"
      ],
      "metadata": {
        "id": "6j6kuTp_37wP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Layer Norm (Normalization Layer)"
      ],
      "metadata": {
        "id": "aqmKyo9gewGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "        Layer Normalization module. (different than batch normalization and better in seq problems)\n",
        "\n",
        "        Tensor          Type            Shape\n",
        "        ===========================================================================\n",
        "        input           long            (batch_size, seq_len, n_embd)\n",
        "        ---------------------------------------------------------------------------\n",
        "        output          float           (batch_size, seq_len, n_embd)\n",
        "        ===========================================================================\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)"
      ],
      "metadata": {
        "id": "LUoCCGNi36n5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Causal Self Attention"
      ],
      "metadata": {
        "id": "OlqdCZome_NA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "        Causal Self-Attention Module\n",
        "        ( why causal? well because attention mechanism is a causal operation not a correlation )\n",
        "\n",
        "        Tensor          Type            Shape\n",
        "        ===========================================================================\n",
        "        input           long            (batch_size, seq_len, n_embd)\n",
        "        ---------------------------------------------------------------------------\n",
        "        output          float           (batch_size, seq_len, n_embd)\n",
        "        ===========================================================================\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0 ; \"n_embd % n_head should be 0.\"\n",
        "        # key, query, and value projections for all heads, but do so in a batch-wise manner.\n",
        "        # in normal Head Attention : nn.Linear(n_embd, head_size), however we are batching all of them\n",
        "\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        # final projection (applied on the output): it concatinate and merge diff info from heads\n",
        "\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        # Flash attention significantly accelerates GPU processing, but requires PyTorch version 2.0 or higher for support.\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: Using the standard attention mechanism. Flash Attention requires PyTorch version 2.0 or higher.\")\n",
        "            # causal mask (communicate with previous tokens only)\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                        .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batchs, Time, channels\n",
        "        # Calculating Q,K,V and performing the multi-head attention in a single matrix operation.\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)   # (B, nh, T, hs) headsize(hs) = n_embd // n_head\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)   # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)   # (B, nh, T, hs)\n",
        "\n",
        "        # Causal self-attention ( computes all self-attention operations ) : (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        if self.flash:\n",
        "            # efficient attention using Flash Attention CUDA kernels\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
        "        else:\n",
        "            # manual implementation of attention mechanism (less efficient)\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v   # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # Multi-HeadAttention : (B, nh, T, hs) ==> (B, T, nh*hs)\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n"
      ],
      "metadata": {
        "id": "Kx8PP9kIWi4a"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####MLP(Multi-Layer Perceptron)"
      ],
      "metadata": {
        "id": "okOcnTEbGH7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "        Multi-Layer Perceptron (MLP): computation layer.\n",
        "        Non-linearity = complex mappings + feature extraction  + ... + processing data (reasoning)\n",
        "\n",
        "        Tensor          Type            Shape\n",
        "        ===========================================================================\n",
        "        input           long            (batch_size, seq_len, n_embd)\n",
        "        ---------------------------------------------------------------------------\n",
        "        output          float           (batch_size, seq_len, n_embd)\n",
        "        ===========================================================================\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "1zYryqXgMDB_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Block(communication + computation)"
      ],
      "metadata": {
        "id": "Hwr9wcWJq59d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "        Transformer Block: communication layer (att mechanism) + computation layer ( feedforward).\n",
        "\n",
        "        Tensor          Type            Shape\n",
        "        ===========================================================================\n",
        "        input           long            (batch_size, seq_len, n_embd)\n",
        "        ---------------------------------------------------------------------------\n",
        "        output          float           (batch_size, seq_len, n_embd)\n",
        "        ===========================================================================\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))   # x = x + f(x) : called residual connection\n",
        "        x = x + self.mlp(self.ln_2(x))    # or skip connections (very crucial in backprop)\n",
        "        return x"
      ],
      "metadata": {
        "id": "nibGUXluMGkf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Config Object"
      ],
      "metadata": {
        "id": "IDjDoa-rrFzV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZD29VGy11LXz"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024      # The maximum sequence length allowed for inputs.\n",
        "    vocab_size: int = 50304     # For GPT-2, it is 50257, and it's padded to the nearest multiple of 64 for efficiency.\n",
        "    n_layer: int = 12           # The number of layers (blocks) in the transformer architecture.\n",
        "    n_head: int = 12            # The number of attention heads in each multi-head attention layer.\n",
        "    n_embd: int = 768           # The size of the embedding dimension for tokens.\n",
        "    dropout: float = 0.0        # The dropout probability to prevent overfitting during training.\n",
        "    bias: bool = True           # Whether to include biases in Linears and LayerNorms;\n",
        "                                # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Body of GPT"
      ],
      "metadata": {
        "id": "fFDaVvLztog3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),                 # Embeddings\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),                 # positional encodings\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),    # Block X N\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)    # proj(n_embd) => vocab_size\n",
        "        # weight tying: could be harmfull in futur (https://paperswithcode.com/method/weight-tying)\n",
        "        # NOTE : this technique is used in auto-encoder: read the paper (https://arxiv.org/pdf/1608.05859v3.pdf)\n",
        "        #        1) This operation requires the number of weights in these layers to be the same.\n",
        "        #        2) We can achieve this by explicitly duplicating the same layer twice.\n",
        "        #        3) Alternatively, as shown below, we can implicitly tie them together.\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        self.apply(self._init_weights)   # init all weights (remember that apply === foreach)\n",
        "\n",
        "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for param_name, param in self.named_parameters():\n",
        "            if param_name.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(param, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters (unit: M)\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Max sequence length is {self.config.block_size}\"\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=device)   # (T)\n",
        "\n",
        "        tok_emb = self.transformer.wte(idx)                         # (B, T) ==> (B, T, C)\n",
        "        pos_emb = self.transformer.wpe(pos)                         # (T) ==> (T, C)\n",
        "\n",
        "        # Performing the forward pass through the entire GPT model.\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            logits = self.lm_head(x)\n",
        "            print(f\"logits : {logits.shape}\")\n",
        "            print(f\"targets : {targets.shape}\")\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            logits = self.lm_head(x[:, [-1], :])    # NOTE: LIST[-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def crop_block_size(self, block_size):\n",
        "        # Model surgery to decrease the block size if necessary for specific use cases.\n",
        "        assert block_size <= self.config.block_size\n",
        "        self.config.block_size = block_size\n",
        "\n",
        "        # Crop the positional embeddings tensor to match the new block size\n",
        "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
        "\n",
        "        # Loop through each transformer block and crop the attention biases if they exist.\n",
        "        for block in self.transformer.h:\n",
        "            if hasattr(block.attn, 'bias'):\n",
        "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type, override_args=None):\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        override_args = override_args or {}                 # empty dict {} by default\n",
        "        assert all(k == 'dropout' for k in override_args)   # Only dropout is modifiable\n",
        "        from transformers import GPT2LMHeadModel\n",
        "\n",
        "        print(f\"loading weights from pretrained gpt: {model_type}\")\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from available model_type\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),    # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024),   # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280),   # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600),   # 1558M params\n",
        "        }[model_type]\n",
        "\n",
        "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
        "        config_args['vocab_size'] = 50257     # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024      # always 1024 for GPT model checkpoints\n",
        "        config_args['bias'] = True            # always True for GPT model checkpoints\n",
        "\n",
        "        # overriding the dropout rate if dropout c override_args\n",
        "        if 'dropout' in override_args:\n",
        "            print(f\"Overriding dropout rate to {override_args['dropout']}\")\n",
        "            config_args['dropout'] = override_args['dropout']\n",
        "\n",
        "        # Re-Initializing GPT Model to fit the specified type requirement\n",
        "        config = GPTConfig(**config_args)  # overriding the the previous config object\n",
        "        model = GPT(config)                # Re-Init model\n",
        "        sd = model.state_dict()            # dict of learnable params\n",
        "        sd_keys = sd.keys()                # list of these params' names\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "        # Initializing the huggingface/transformers Pre-Trained model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # Our-Model params = Copy(Pre-Trained Model params)\n",
        "        # Note : parameters most be aligned have same names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]   # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]          # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "\n",
        "        # The OpenAI checkpoints use \"Conv1D\" module, but we are using a simple Linear layer.\n",
        "        # Therefore, we have to transpose the weights when importing the checkpoints.\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}                 # All params\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}   # filter those who did not require grad\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]         # i.e. weight tensors( matmuls + embeddings + ...)\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]        # i.e. layer norms\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
        "        \"\"\" Model Flops Utilization rate = empirical performance / peak performance \"\"\"\n",
        "        # Model Flops Utilization (MFU): It is a metric used to measure how efficiently\n",
        "        # a deep learning model utilizes the available computational resources.\n",
        "        N = self.get_num_params()\n",
        "        cfg = self.config\n",
        "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
        "        flops_per_token = 6*N + 12*L*H*Q*T\n",
        "        flops_per_fwdbwd = flops_per_token * T\n",
        "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
        "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
        "        flops_achieved = flops_per_iter * (1.0/dt)    # per second (actual flops)\n",
        "        flops_promised = 312e12                       # A100 GPU bfloat16 peak flops is 312 TFLOPS (theoretical flops)\n",
        "        mfu = flops_achieved / flops_promised\n",
        "        return mfu\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        for _ in range(max_new_tokens):\n",
        "            # If the conditioning sequence exceeds the 'block_size', crop it to the most recent 'block_size' tokens.\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "\n",
        "            # Forward pass through the model to obtain logits for the next token in the sequence.\n",
        "            logits, _ = self(idx_cond)\n",
        "\n",
        "            # Scale the logits at the final step by the 'temperature' to control the randomness of token sampling.\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            # Optionally crop the logits to only the top k options, ensuring more focused sampling.\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            # Apply the softmax function to convert logits into probabilities for sampling.\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # Sample from the probability distribution to get the next token indices.\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Append the sampled index to the running sequence and continue generation.\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ],
      "metadata": {
        "id": "9QA8gNAZ1ftY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initializing Model and Train it"
      ],
      "metadata": {
        "id": "5Ek0m7bDvOYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Model Configuration and Hyperparameters"
      ],
      "metadata": {
        "id": "q2W6KSDaEvBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#---------------------(NOTE: this is gpt-2 configuration)----------------------#\n",
        "checkpts_dir = 'check_points'\n",
        "eval_interval = 2000\n",
        "log_interval = 1\n",
        "eval_iters = 200\n",
        "init_from = 'scratch'                  # ['scratch', 'resume', 'gpt2*']\n",
        "save_checkpoint = True\n",
        "eval_only = False\n",
        "\n",
        "#-------------------------------(wandb logging)--------------------------------#\n",
        "wandb_log = True\n",
        "wandb_project = 'owt'\n",
        "wandb_run_name = 'gpt2'                # 'run' + str(time.time())\n",
        "\n",
        "#----------------------------------(data)--------------------------------------#\n",
        "db_name = 'openwebtext'\n",
        "grad_accum_steps = 5 * 8        # used to simulate larger batch sizes\n",
        "batch_size = 12                 # grad_accum_steps>1 => micro-batch-size=12\n",
        "block_size = 1024\n",
        "\n",
        "#----------------------------------(model)-------------------------------------#\n",
        "n_layer = 12\n",
        "n_head = 12\n",
        "n_embd = 768\n",
        "dropout = 0.0   # NOTE: while fine-tuning it is recommanded to use dropout=0.1+\n",
        "bias = False\n",
        "\n",
        "#----------------------------(adamw optimizer)---------------------------------#\n",
        "learning_rate = 6e-4\n",
        "max_iters = 600000\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "grad_clip = 1.0                 # set(0.0) ==> disable clipping\n",
        "\n",
        "#---------------------------------(lr decay)-----------------------------------#\n",
        "decay_lr = True\n",
        "warmup_iters = 2000\n",
        "lr_decay_iters = 600000     # ~= max_iters per 'Chunk of Data'\n",
        "min_lr = 6e-5               # ~= learning_rate/10 per 'Chunk of Data'\n",
        "\n",
        "#--------------------------------(system)--------------------------------------#\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'   # ['cpu', 'cuda', 'cuda:0', 'cuda:1', ... 'mps']\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'   # ['float32', 'bfloat16', 'float16']\n",
        "compile = True      # use PyTorch 2.0 to compile the model to be faster"
      ],
      "metadata": {
        "id": "aDZ1eN2qE0H8"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Data Loader\n",
        "\n",
        "<center>\n",
        "  <h6>Loading Data By chuncks using NumPy Memory Mapped Array</h6>\n",
        "  <img src=\"https://github-production-user-asset-6210df.s3.amazonaws.com/89405673/259172272-6434381c-b51a-4974-ba66-399ed7fc5778.png\" />\n",
        "</center>"
      ],
      "metadata": {
        "id": "yXRq4gNoFf1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "#                                 Data Loader                                  #\n",
        "#                Note: np.memmap loads the data when it is needed              #\n",
        "################################################################################\n",
        "data_dir = os.path.join('/content/drive/MyDrive/GPT_FROM_SCRATCH', db_name)\n",
        "# train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "# val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "\n",
        "\n",
        "# def get_batch(split):\n",
        "#     data = train_data if split == 'train' else val_data\n",
        "#     ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "#     x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "#     y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "#     if device == 'cuda':\n",
        "#         # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "#         x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "#     else:\n",
        "#         x, y = x.to(device), y.to(device)\n",
        "#     return x, y\n",
        "\n",
        "\n",
        "#======> just to test the model in a tiny dataset\n",
        "def get_batch(data):\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
        "    if device == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "VE_ClAdVFh-2"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Initialization\n",
        "\n",
        "<center>\n",
        "  <h6>AUTOMATIC MIXED PRECISION</h6>\n",
        "  <img src=\"https://github-production-user-asset-6210df.s3.amazonaws.com/89405673/259163442-802ef1b4-8130-44f5-86be-90e34dcd437e.png\" width=\"900\" height=\"200\"/>\n",
        "  <img src=\"https://github-production-user-asset-6210df.s3.amazonaws.com/89405673/259163449-0fd88fd1-3c32-4b79-807e-e4e281819a13.png\" width=\"500\" height=\"140\"/>\n",
        "</center>"
      ],
      "metadata": {
        "id": "W-aCol3iE1KJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(checkpts_dir, exist_ok=True)            # Create checkpts dir if not exist\n",
        "torch.manual_seed(2000)                             # Reproducibility\n",
        "torch.backends.cuda.matmul.allow_tf32 = True        # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True              # allow tf32 on cudnn\n",
        "\n",
        "# NOTE: float16 data type will automatically use a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device == 'cpu' else torch.amp.autocast(device_type=device, dtype=ptdtype)\n",
        "\n",
        "\n",
        "#----------------------[Note: init_from='resume' or ...)]----------------------#\n",
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "init_from = 'scratch'\n",
        "\n",
        "\n",
        "#-----------------------------[Restore Vocab_Size]-----------------------------#\n",
        "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
        "meta_vocab_size = None\n",
        "if os.path.exists(meta_path):\n",
        "    with open(meta_path, 'rb') as f:\n",
        "        meta = pickle.load(f)\n",
        "    meta_vocab_size = meta['vocab_size']\n",
        "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
        "\n",
        "\n",
        "#------------------------------[Init Our Model]--------------------------------#\n",
        "model_args = dict(n_layer=n_layer,\n",
        "                  n_head=n_head,\n",
        "                  n_embd=n_embd,\n",
        "                  block_size=block_size,\n",
        "                  bias=bias,\n",
        "                  vocab_size=None,\n",
        "                  dropout=dropout)\n",
        "\n",
        "\n",
        "#---------------[Building Model as specified here 'init_from' ]----------------#\n",
        "print(f\"Initializing The Model from : {init_from}\")\n",
        "if init_from == 'scratch':\n",
        "    # NOTE: GPT-2 original vocab size is 50,257 but for effeciency we are using 50,304\n",
        "    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)\n",
        "elif init_from == 'resume':\n",
        "    ckpt_path = os.path.join(checkpts_dir, 'ckpt.pt')\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    checkpoint_model_args = checkpoint['model_args']\n",
        "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
        "        model_args[k] = checkpoint_model_args[k]\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)\n",
        "    state_dict = checkpoint['model']\n",
        "    # NOTE: checkpoints sometimes get _orig_mod prefix :( \"i will fix it later\"\n",
        "    unwanted_prefix = '_orig_mod.'\n",
        "    for key, value in list(state_dict.items()):\n",
        "        if key.startswith(unwanted_prefix):\n",
        "            state_dict[key[len(unwanted_prefix):]] = state_dict.pop(key)\n",
        "    model.load_state_dict(state_dict)\n",
        "    iter_num = checkpoint['iter_num']\n",
        "    best_val_loss = checkpoint['best_val_loss']\n",
        "elif init_from.startswith('gpt2'):\n",
        "    override_args = dict(dropout=dropout)  # initialize from OpenAI GPT-2 weights\n",
        "    model = GPT.from_pretrained(init_from, override_args)\n",
        "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
        "        model_args[k] = getattr(model.config, k)\n",
        "else:\n",
        "    print(\"Oups, There is no such option !!\")\n",
        "\n",
        "\n",
        "#------------------[Crop block size of the original model]---------------------#\n",
        "if block_size < model.config.block_size:\n",
        "    model.crop_block_size(block_size)\n",
        "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
        "model.to(device)\n",
        "\n",
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "\n",
        "print(f\"scaler : {scaler}\")\n",
        "\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device)\n",
        "if init_from == 'resume':\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "checkpoint = None   # free up memory\n",
        "\n",
        "if compile:\n",
        "    print(\"Compiling the model... (Estimated duration: ~= 1min)\")\n",
        "    model = torch.compile(model) # requires PyTorch 2.0\n",
        "\n",
        "# if wandb_log:\n",
        "#     wandb.init(project=wandb_project, name=wandb_run_name, config=config)"
      ],
      "metadata": {
        "id": "2qkNNUvjvNrQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02134526-3c3c-4248-a33f-2a662478bd93"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing The Model from : scratch\n",
            "number of parameters: 123.59M\n",
            "scaler : <torch.cuda.amp.grad_scaler.GradScaler object at 0x7a817cb08310>\n",
            "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
            "num non-decayed parameter tensors: 25, with 19,200 parameters\n",
            "using fused AdamW: True\n",
            "Compiling the model... (Estimated duration: ~= 1min)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Utils\n",
        "\n",
        "\n",
        "<center>\n",
        "  <h6><strong>Learning Rate Graph</strong> ( Linear -> Cosine Decay -> lr_min )</h6>\n",
        "  <img src=\"https://github-production-user-asset-6210df.s3.amazonaws.com/89405673/259077879-acb08446-83da-46ea-86ea-d4d4f3e332ac.png\" />\n",
        "</center>"
      ],
      "metadata": {
        "id": "YtrbXkYB9q1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "#                   Estimation of Loss for Arbitrary Batches                   #\n",
        "################################################################################\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#         Utilizing a Cosine Learning Rate Decay Scheduler with Warmup         #\n",
        "################################################################################\n",
        "def get_lr(it):\n",
        "    w = warmup_iters\n",
        "    lr = learning_rate\n",
        "    lr_di = lr_decay_iters\n",
        "    #-------------------[ Linear Warmup ]------------------#\n",
        "    if it < w:\n",
        "        return lr * it / w\n",
        "    #-------------------[ Enough WarmUp ]------------------#\n",
        "    if it > lr_di:\n",
        "        return min_lr\n",
        "    #---------[ w < Cosine_Decay(min_lr) < lr_di ]---------#\n",
        "    decay_ratio = (it - w) / (lr_di - w)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return min_lr + coeff * (lr - min_lr)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#                                 Save Checkpoints                             #\n",
        "################################################################################\n",
        "def save_checkpts(model, optimizer, model_args, iter_num, best_val_loss, config):\n",
        "      print(\"Saving checkpoints ...\")\n",
        "      checkpoint = {\n",
        "          'model': raw_model.state_dict(),\n",
        "          'optimizer': optimizer.state_dict(),\n",
        "          'model_args': model_args,\n",
        "          'iter_num': iter_num,\n",
        "          'best_val_loss': best_val_loss,\n",
        "          'config': config,\n",
        "          }\n",
        "      print(\"Checkpoints Saved Successfully :)\")\n",
        "      torch.save(checkpoint, os.path.join(checkpts_dir, 'ckpt.pt'))\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#                                   Evaluate Model                             #\n",
        "################################################################################\n",
        "def evaluate_model(iter_num, eval_interval, best_val_loss, save_checkpoint):\n",
        "    if iter_num % eval_interval == 0 and False:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if wandb_log:\n",
        "            wandb.log({\n",
        "                \"iter\": iter_num,\n",
        "                \"train/loss\": losses['train'],\n",
        "                \"val/loss\": losses['val'],\n",
        "                \"lr\": lr,\n",
        "                \"mfu\": running_mfu*100,\n",
        "            })\n",
        "        if losses['val'] < best_val_loss or save_checkpoint:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                save_checkpts(\n",
        "                    model=raw_model.state_dict(),\n",
        "                    optimizer=optimizer.state_dict(),\n",
        "                    model_args=model_args,\n",
        "                    iter_num=iter_num,\n",
        "                    best_val_loss=best_val_loss,\n",
        "                    config=config\n",
        "                )"
      ],
      "metadata": {
        "id": "dZRCWIMf9ua5"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Training Loop"
      ],
      "metadata": {
        "id": "MwQv0C-hhewh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# X, Y = get_batch('train') # fetch the very first batch\n",
        "X, Y = get_batch(text_data)\n",
        "t0 = time.time()\n",
        "local_iter_num = 0\n",
        "raw_model =  model\n",
        "running_mfu = -1.0\n",
        "\n",
        "while True:\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluation (Train / val)\n",
        "\n",
        "\n",
        "\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(grad_accum_steps):\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / grad_accum_steps\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch('train')\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0:\n",
        "        # get loss as float. note: this is a CPU-GPU sync point\n",
        "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
        "        lossf = loss.item() * grad_accum_steps\n",
        "        if local_iter_num >= 5: # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(batch_size * grad_accum_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break"
      ],
      "metadata": {
        "id": "IxRewAPnhiqR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "cad24e4b-fba0-4d7c-b830-6d998fd418e3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits : (12, 1024, 50304)\n",
            "targets : (12, 1024)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-035d15fed261>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mgrad_accum_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# immediately async prefetch next batch while model is doing the forward pass on the GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;31m# backward pass, with gradient scaling if training in fp16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-2a3f22d4ca7d>\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#======> just to test the model in a tiny dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: random_ expects 'from' to be less than 'to', but got from=0 >= to=-1019"
          ]
        }
      ]
    }
  ]
}
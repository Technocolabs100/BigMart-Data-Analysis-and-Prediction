# -*- coding: utf-8 -*-
"""EsraaMaged-MiniProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11TsPmGrPoWv8h9n-lW3mVUIynYc86h-m

# **<<<<< BigMart Data Analysis andPrediction >>>>>**

The data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store have been defined. The aim of this data science project is to build a predictive model and find out the sales of each product at a particular store.
Using this model, BigMart will try to understand the properties of products and stores which play a key role in increasing sales.
 The data has missing values as some stores do not report all the data due to technical glitches. Hence, it will be required to treat them accordingly.

# **Loading Packages and Data**

## **Import libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn.linear_model
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.metrics import mean_absolute_error

"""## **Load the data**"""

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv('/content/drive/MyDrive/Train.csv')

"""## **Data Structure and Content**"""



"""## **Exploratory Data Analysis**"""

print("Number of rows and columns:", data.shape)

print("Summary statistics of numerical columns:")
print(data.describe())

print("Unique values in each column:")
print(data.nunique())

"""### **Check for unique values**"""

print(data['Item_Fat_Content'].unique())
print(data['Item_Type'].unique())
print(data['Outlet_Establishment_Year'].sort_values().unique())
print(data['Outlet_Location_Type'].unique())
print(data['Outlet_Type'].unique())

"""### **Check for outliers**"""

data.plot(kind='box', subplots=True, layout=(1,5), figsize=(20,5))

"""### **Remove outliers**"""

data = data.drop(data[data['Item_Weight'] > 1000].index)

data.plot(kind='box', subplots=True, layout=(1,5), figsize=(20,5))

"""## **Univariate Analysis**

### **Plotting distribution of numerical variables**
"""

num_cols = ['Item_Weight', 'Item_Visibility', 'Item_MRP']
for column in num_cols:
    sns.histplot(data[column].dropna())
    plt.title(column)
    plt.show()

"""### **Plotting count of categorical variables**"""

cat_cols = ['Item_Fat_Content', 'Item_Type', 'Outlet_Identifier', 'Outlet_Establishment_Year', 'Outlet_Size',
            'Outlet_Location_Type', 'Outlet_Type']

for column in cat_cols:
  sns.histplot(data[column].dropna())
  plt.title(column)
  plt.xticks(rotation=90)
  plt.show()

"""## **Bivariate Analysis**

### **Scatterplot of numerical variables with target variable**
"""

sns.scatterplot(x='Item_Weight', y='Item_Outlet_Sales', data=data)
plt.title("Item_Weight vs Item_Outlet_Sales")
plt.show()

"""### **Correlation matrix of numerical variables**"""

corr_matrix = data[num_cols].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title("Correlation Matrix")
plt.show()

"""## **Missing Value Treatment**

### **Check for missing values**
"""

print(data.isnull().sum())

"""### **Fill missing values in numerical columns with mean**"""

data['Item_Weight'].fillna(data['Item_Weight'].mean(), inplace=True)

"""
### **Fill missing values in categorical columns with mode**"""

data['Outlet_Size'].fillna(data['Outlet_Size'].mode()[0], inplace=True)

"""## **Feature Engineering**

### **Create new feature: Item_Type_Category**
"""

data['Item_Type_Category'] = data['Item_Type'].apply(lambda x: x.split('_')[0])

data['Item_Fat_Content'] = data.Item_Fat_Content.replace(['LF', 'low fat', 'reg','Low Fat','Low Fat', 'Regular'])
data.Item_Fat_Content.value_counts()

le = LabelEncoder()
label =  ['Item_Fat_Content', 'Item_Type', 'Outlet_Identifier', 'Outlet_Establishment_Year', 'Outlet_Size',
            'Outlet_Location_Type', 'Outlet_Type']
for i in label:
    data[i] = le.fit_transform(data[i])
data.head()

"""## **Encoding Categorical Variables**"""

categorical_columns = [column for column in data.columns if data[column].dtype.name == "category"]

"""## **Label encoding**"""

label_encoder = LabelEncoder()
for column in categorical_columns:
    data[column] = label_encoder.fit_transform(data[column])

data.head()

"""## **Preprocessing**"""

data.drop(['Item_Type_Category','Item_Identifier'],axis=1,inplace=True)

y = data['Item_Outlet_Sales']
X = data.drop('Item_Outlet_Sales', axis=1)

data.head()

"""### **Split data into train and test sets**"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

"""## **Modeling**

### **Linear Regression**
"""

linear_reg = LinearRegression()
linear_reg.fit(X_train, y_train)
y_pred = linear_reg.predict(X_test)

linear_reg_rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("Linear Regression RMSE:", linear_reg_rmse)

linear_reg_mae = mean_absolute_error(y_test, y_pred)
print("Linear Regression MAE:", linear_reg_mae)

"""### **Regularized Linear Regression (Ridge)**"""

ridge_reg = Ridge(alpha=0.5)
ridge_reg.fit(X_train, y_train)
y_pred = ridge_reg.predict(X_test)

ridge_reg_rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("Ridge Regression RMSE:", ridge_reg_rmse)

ridge_reg_mae = mean_absolute_error(y_test, y_pred)
print("Ridge Regression MAE:", ridge_reg_mae)

"""### **Regularized Linear Regression (Lasso)**"""

lasso_reg = Lasso(alpha=0.5)
lasso_reg.fit(X_train, y_train)
y_pred = lasso_reg.predict(X_test)
lasso_reg_rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("Lasso Regression RMSE:", lasso_reg_rmse)

lasso_reg_mae = mean_absolute_error(y_test, y_pred)
print("Lasso Regression MAE:", lasso_reg_mae)

"""### **Random Forest**"""

rf_reg = RandomForestRegressor(n_estimators=100)
rf_reg.fit(X_train, y_train)
y_pred = rf_reg.predict(X_test)
rf_reg_rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("Random Forest RMSE:", rf_reg_rmse)


rf_reg_mae = mean_absolute_error(y_test, y_pred)
print("Random Forest MAE:", rf_reg_mae)

"""### **XGBoost**"""

xgb_reg = XGBRegressor(n_estimators=100)
xgb_reg.fit(X_train, y_train)
y_pred = xgb_reg.predict(X_test)
xgb_reg_rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("XGBoost RMSE:", xgb_reg_rmse)

xgb_reg_mae = mean_absolute_error(y_test, y_pred)
print("XGBoost MAE:", xgb_reg_mae)

"""Summarty

In this Project:

**First**, we load the necessary packages and import the database.

We **then** proceed to explore the data visually and statistically to gain insights and understand the relationships between variables.

**Following that**, we analyze individual variables to understand their distribution and characteristics (univariate analysis) and analyze the relationships between pairs of variables to identify correlations and patterns (bivariate analysis).

We **then** handle missing values in the database by either imputing them or removing the corresponding rows/columns.

**After that**, we create new features from the existing variables to improve the model's performance through feature engineering.

**Next**, we convert categorical variables into numerical representations to facilitate modeling through encoding.
This can involve label encoding, where we assign unique numerical labels to each category within a variable, or one hot encoding, where we convert categorical variables with multiple categories into binary columns.
We preprocess the data by scaling, normalizing, or transforming variables to meet modeling requirements.

**Finally**, we apply various machine learning models to the preprocessed data, such as linear regression, regularized linear regression algorithms like Ridge or Lasso, random forest, and XGBoost, to predict outcomes.
"""